{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8c104e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130f9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch_setupParameters import set_seed,set_device, getDefaultRNNArgs\n",
    "from torch_dataPreprocessing import loadAllRealDatasets, prepareDataCubesForRNN\n",
    "from torch_dataPreprocessing import normalizeSentenceDataCube, binTensor\n",
    "from torch_dataPreprocessing import handBCI_Dataset, handBCI_SythDataset, gaussSmooth\n",
    "from tfrecord.torch.dataset import MultiTFRecordDataset\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c3126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n",
      "GPU is enabled !\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d03788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/RNNTrainingSteps/HeldOutTrials'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['outputDir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abf0c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 8\n",
      "synthBatchSize: 3\n"
     ]
    }
   ],
   "source": [
    "#point this towards the top level dataset directory\n",
    "rootDir = '../handwritingBCIData/'\n",
    "outDir = 'output/'\n",
    "#train an RNN using data from these specified sessions\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "\n",
    "#use this train/test partition \n",
    "cvPart = 'HeldOutTrials'\n",
    "\n",
    "#name of the directory where this RNN run will be saved\n",
    "rnnOutputDir = cvPart\n",
    "\n",
    "## parameters\n",
    "args = getDefaultRNNArgs(rootDir, cvPart, outDir)\n",
    "#Configure the arguments for a multi-day RNN (that will have a unique input layer for each day)\n",
    "for x in range(len(dataDirs)):\n",
    "    args['sentencesFile_'+str(x)] = '/media/rich/bigSSD/NMA_data/Willet_data/Datasets/'+dataDirs[x]+'/sentences.mat'\n",
    "    args['singleLettersFile_'+str(x)] = '/media/rich/bigSSD/NMA_data/Willet_data/Datasets/'+dataDirs[x]+'/singleLetters.mat'\n",
    "    args['labelsFile_'+str(x)] = '/media/rich/bigSSD/NMA_data/Willet_data/RNNTrainingSteps/Step2_HMMLabels/'+cvPart+'/'+dataDirs[x]+'_timeSeriesLabels.mat'\n",
    "    args['syntheticDatasetDir_'+str(x)] = '/media/rich/bigSSD/NMA_data/Willet_data/Datasets/'+dataDirs[x]+'/'+cvPart+'/'+dataDirs[x]+'_syntheticSentences/'\n",
    "    args['cvPartitionFile_'+str(x)] = '/media/rich/bigSSD/NMA_data/Willet_data/RNNTrainingSteps/trainTestPartitions_'+cvPart+'.mat'\n",
    "    args['sessionName_'+str(x)] = dataDirs[x]\n",
    "args['outputDir'] = '/media/rich/bigSSD/NMA_data/Willet_data/RNN_outputs'\n",
    "\n",
    "for t in range(30):  ## 10 days\n",
    "    if 'labelsFile_'+str(t) not in args.keys():\n",
    "        args['nDays'] = t\n",
    "        break\n",
    "if not os.path.isdir(args['outputDir']):\n",
    "    os.mkdir(args['outputDir'])\n",
    "    \n",
    "#this weights each day equally (0.1 probability for each day) and allocates a unique input layer for each day (0-9)\n",
    "args['dayProbability'] = '[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]'\n",
    "args['dayToLayerMap'] = '[0,1,2,3,4,5,6,7,8,9]'\n",
    "# args['verbose'] = True ## extra print-out information\n",
    "\n",
    "args['mode'] = 'train' ## make sure it is set in 'train' mode\n",
    "print('batchSize:', args['batchSize'])\n",
    "print('synthBatchSize:', args['synthBatchSize'])\n",
    "args['ForTestingOnly'] = False ## FOR DEBUGING. set \"self.nDays = 2\" (use 2 days of data for testing run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f0204",
   "metadata": {},
   "source": [
    "## torch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define which datasets to process\n",
    "dataDirs = ['t5.2019.05.08',\n",
    "            't5.2019.11.25',\n",
    "            't5.2019.12.09',\n",
    "            't5.2019.12.11',\n",
    "            't5.2019.12.18',\n",
    "            't5.2019.12.20',\n",
    "            't5.2020.01.06',\n",
    "            't5.2020.01.08',\n",
    "            't5.2020.01.13',\n",
    "#             't5.2020.01.15'\n",
    "           ]\n",
    "cvParts = ['HeldOutBlocks', 'HeldOutTrials']\n",
    "\n",
    "n_epochs = 4\n",
    "n_days = len(dataDirs)\n",
    "n_bats = 20\n",
    "batch_size = 256\n",
    "val_fraction = 0.2\n",
    "\n",
    "win_len = 500\n",
    "\n",
    "n_electrodes = 192\n",
    "n_letters = 32\n",
    "\n",
    "dir_folders = Path(r'/media/rich/bigSSD/NMA_data/Willet_data/Datasets/').resolve()\n",
    "\n",
    "DEVICE = set_device()\n",
    "net = Net().to(DEVICE)\n",
    "for ii, layer in enumerate(net.parameters()):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layer, gain=nn.init.calculate_gain('relu'))\n",
    "criterion_letters = nn.CrossEntropyLoss()\n",
    "criterion_onsets = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), \n",
    "                             lr=0.0002, \n",
    "#                              betas=(0.9, 0.999), \n",
    "#                              eps=1e-08, \n",
    "                             weight_decay=0.0001, \n",
    "                             amsgrad=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae0baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real data  2\n",
      "create real dataset  2\n",
      "processing sythetic data  2\n",
      "Loading real data  3\n",
      "create real dataset  3\n",
      "processing sythetic data  3\n"
     ]
    }
   ],
   "source": [
    "allSynthDataLoaders = []\n",
    "allRealDataLoaders = []\n",
    "allValDataLoaders = []\n",
    "daysWithValData = []\n",
    "args['isTraining'] = True\n",
    "for dayIdx in [2,3]: #range(args['nDays']):\n",
    "    ## real data\n",
    "    print('Loading real data ', dayIdx)\n",
    "    neuralData, targets, errWeights, binsPerTrial, cvIdx = prepareDataCubesForRNN(args['sentencesFile_'+str(dayIdx)],\n",
    "                                                                          args['singleLettersFile_'+str(dayIdx)],\n",
    "                                                                          args['labelsFile_'+str(dayIdx)],\n",
    "                                                                          args['cvPartitionFile_'+str(dayIdx)],\n",
    "                                                                          args['sessionName_'+str(dayIdx)],\n",
    "                                                                          args['rnnBinSize'],\n",
    "                                                                          args['timeSteps'],\n",
    "                                                                          args['isTraining'])\n",
    "    realDataSize = args['batchSize'] - args['synthBatchSize']\n",
    "    trainIdx = cvIdx['trainIdx']\n",
    "    valIdx = cvIdx['testIdx']\n",
    "    print('create real dataset ', dayIdx)\n",
    "    realData_train = handBCI_Dataset(args,neuralData[trainIdx,:,:], targets[trainIdx,:,:], errWeights[trainIdx,:],\\\n",
    "                               binsPerTrial[trainIdx,np.newaxis],\\\n",
    "                               addNoise=True)\n",
    "    realDataTrain_Loader = torch.utils.data.DataLoader(realData_train, batch_size =realDataSize,shuffle=True, num_workers=0)\n",
    "    \n",
    "    if len(valIdx)==0:\n",
    "        realDataVal_Loader = realDataTrain_Loader\n",
    "    else:\n",
    "        realData_val = handBCI_Dataset(args,neuralData[valIdx,:,:], targets[valIdx,:,:], errWeights[valIdx,:],\\\n",
    "                                   binsPerTrial[valIdx,np.newaxis],\\\n",
    "                                       addNoise=False)\n",
    "        realDataVal_Loader = torch.utils.data.DataLoader(realData_val, batch_size =args['batchSize'],shuffle=True, num_workers=0)\n",
    "        daysWithValData.append(dayIdx)\n",
    "    allRealDataLoaders.append(realDataTrain_Loader)\n",
    "    allValDataLoaders.append(realDataVal_Loader)\n",
    "              \n",
    "    ## sythetic data\n",
    "    print('processing sythetic data ', dayIdx)\n",
    "    synthDir = args['syntheticDatasetDir_'+str(dayIdx)]\n",
    "    synth_obj = handBCI_SythDataset(synthDir, args)\n",
    "    synth_ds = synth_obj.makeDataSet()\n",
    "    synth_loader = torch.utils.data.DataLoader(synth_ds, batch_size=args['synthBatchSize'])\n",
    "    allSynthDataLoaders.append(synth_loader)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42a6cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class charSeqNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(charSeqNet, self).__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        #count how many days of data are specified\n",
    "        self.nDays = args['nDays']\n",
    "        self.args = args\n",
    "        if self.args['seed']==-1:\n",
    "            self.args['seed']=datetime.now().microsecond\n",
    "        drop_prob = args['drop_prob']\n",
    "        #define the dimensions of layers in the RNN\n",
    "        nOutputs = 31\n",
    "        nInputs = 192        \n",
    "        nUnits = args['nUnits']\n",
    "        nTimeSteps = args['timeSteps']\n",
    "        self.rnnBinSize = args['rnnBinSize']\n",
    "        inputLayers = []        \n",
    "#        shape: [args['batchSize'], args['timeSteps'], nInputs]\n",
    "        for j in range(self.nDays):\n",
    "            inputLayers.append(nn.Linear(nInputs, nInputs, bias = True))\n",
    "\n",
    "        self.inputLayers = inputLayers\n",
    "        self.gru1 = torch.nn.GRU(nInputs, nUnits, 1, \\\n",
    "                                 batch_first=True, dropout=drop_prob)\n",
    " \n",
    "        self.gru2 = torch.nn.GRU(nUnits, nUnits, 1, \\\n",
    "                                 batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        self.fc1 = nn.Linear(nUnits, nOutputs, bias = True)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "    def forward(self, x, dayIdx):\n",
    "\n",
    "        if self.args['smoothInputs']==1: ## smooth\n",
    "            x = torch.Tensor(gaussSmooth(x, kernelSD=4/self.rnnBinSize))\n",
    "        layer1 = self.inputLayers[dayIdx].to('cuda') ## day specific input layer\n",
    "        x = layer1(x.to('cuda'))\n",
    "        x, h = self.gru1(x)\n",
    "        ## TODO: downsample x's time dimension\n",
    "        x, h = self.gru2(x)\n",
    "        ## TODO: upsample\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def train(self):\n",
    "        ## TO be continued ...\n",
    "        n = 100\n",
    "        for epoc in range(n):\n",
    "            dayIdx = np.random.randint(self.nDays)\n",
    "            miniBatch = next(iter(allRealDataLoaders[0])) ## change 0 to dayIdx in full mode\n",
    "            X = miniBatch['inputs']\n",
    "            target = miniBatch['labels'].to(DEVICE).long()\n",
    "            er = miniBatch['errWeights']\n",
    "            output = self.forward(X, dayIdx)\n",
    "            ## output is 32 dimensions, target is 31?\n",
    "            loss = self.criterion(output, target[:,-1,:31])\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb5129d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# miniBatch = next(iter(allRealDataLoaders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af22d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Network    2676255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rich/OS/Users/Richard/Linux_stuff_on_OS/conda_envs/envs/rapids-0.19/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "## Uncomment the lines below to train your network\n",
    "charSeq_net = charSeqNet(args).to(DEVICE)\n",
    "print(\"Total Parameters in Network {:10d}\".format(sum(p.numel() for p in charSeq_net.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3d9574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rich/OS/Users/Richard/Linux_stuff_on_OS/conda_envs/envs/rapids-0.19/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/media/rich/Home_Linux_partition/github_repos/NMA-project/MH_handBCI_pytorch/torch_dataPreprocessing.py:208: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data[:,:,np.newaxis]).transpose(1,2)\n"
     ]
    }
   ],
   "source": [
    "charSeq_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774be1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
