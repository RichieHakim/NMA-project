{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch_setupParameters import set_seed,set_device, getDefaultRNNArgs\n",
    "from torch_dataPreprocessing import loadAllRealDatasets, prepareDataCubesForRNN\n",
    "from torch_dataPreprocessing import normalizeSentenceDataCube, binTensor\n",
    "from torch_dataPreprocessing import handBCI_Dataset, handBCI_SythDataset, combineSynthAndReal, gaussSmooth\n",
    "from tfrecord.torch.dataset import MultiTFRecordDataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary as netSummary\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_dataPreprocessing\n",
    "# importlib.reload(torch_dataPreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n",
      "GPU is enabled !\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 32\n",
      "synthBatchSize: 12\n"
     ]
    }
   ],
   "source": [
    "#point this towards the top level dataset directory\n",
    "rootDir = '../handwritingBCIData/'\n",
    "outDir = 'output/'\n",
    "#train an RNN using data from these specified sessions\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "\n",
    "#use this train/test partition \n",
    "cvPart = 'HeldOutTrials'\n",
    "\n",
    "#name of the directory where this RNN run will be saved\n",
    "rnnOutputDir = cvPart\n",
    "\n",
    "## parameters\n",
    "args = getDefaultRNNArgs(rootDir, cvPart, outDir)\n",
    "#Configure the arguments for a multi-day RNN (that will have a unique input layer for each day)\n",
    "for x in range(len(dataDirs)):\n",
    "    args['sentencesFile_'+str(x)] = rootDir+'Datasets/'+dataDirs[x]+'/sentences.mat'\n",
    "    args['singleLettersFile_'+str(x)] = rootDir+'Datasets/'+dataDirs[x]+'/singleLetters.mat'\n",
    "    args['labelsFile_'+str(x)] = rootDir+'RNNTrainingSteps/Step2_HMMLabels/'+cvPart+'/'+dataDirs[x]+'_timeSeriesLabels.mat'\n",
    "    args['syntheticDatasetDir_'+str(x)] = rootDir+'Datasets/'+dataDirs[x]+'/'+cvPart+'/'+dataDirs[x]+'_syntheticSentences/'\n",
    "    args['cvPartitionFile_'+str(x)] = rootDir+'RNNTrainingSteps/trainTestPartitions_'+cvPart+'.mat'\n",
    "    args['sessionName_'+str(x)] = dataDirs[x]\n",
    "\n",
    "for t in range(30):  ## 10 days\n",
    "    if 'labelsFile_'+str(t) not in args.keys():\n",
    "        args['nDays'] = t\n",
    "        break\n",
    "if not os.path.isdir(args['outputDir']):\n",
    "    os.mkdir(args['outputDir'])\n",
    "    \n",
    "#this weights each day equally (0.1 probability for each day) and allocates a unique input layer for each day (0-9)\n",
    "args['dayProbability'] = '[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]'\n",
    "args['dayToLayerMap'] = '[0,1,2,3,4,5,6,7,8,9]'\n",
    "# args['verbose'] = True ## extra print-out information\n",
    "\n",
    "args['mode'] = 'train' ## make sure it is set in 'train' mode\n",
    "print('batchSize:', args['batchSize'])\n",
    "print('synthBatchSize:', args['synthBatchSize'])\n",
    "args['ForTestingOnly'] = False ## FOR DEBUGING. set \"self.nDays = 2\" (use 2 days of data for testing run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real data  0\n",
      "create real dataset  0\n",
      "processing sythetic data  0\n",
      "Loading real data  1\n",
      "create real dataset  1\n",
      "processing sythetic data  1\n",
      "Loading real data  2\n",
      "create real dataset  2\n",
      "processing sythetic data  2\n",
      "Loading real data  3\n",
      "create real dataset  3\n",
      "processing sythetic data  3\n",
      "Loading real data  4\n",
      "create real dataset  4\n",
      "processing sythetic data  4\n",
      "Loading real data  5\n",
      "create real dataset  5\n",
      "processing sythetic data  5\n",
      "Loading real data  6\n",
      "create real dataset  6\n",
      "processing sythetic data  6\n",
      "Loading real data  7\n",
      "create real dataset  7\n",
      "processing sythetic data  7\n",
      "Loading real data  8\n",
      "create real dataset  8\n",
      "processing sythetic data  8\n",
      "Loading real data  9\n",
      "create real dataset  9\n",
      "processing sythetic data  9\n"
     ]
    }
   ],
   "source": [
    "allSynthDataLoaders = []\n",
    "allRealDataLoaders = []\n",
    "allValDataLoaders = []\n",
    "daysWithValData = []\n",
    "args['isTraining'] = True\n",
    "for dayIdx in range(args['nDays']):\n",
    "    ## real data\n",
    "    print('Loading real data ', dayIdx)\n",
    "    neuralData, targets, errWeights, binsPerTrial, cvIdx = prepareDataCubesForRNN(args['sentencesFile_'+str(dayIdx)],\n",
    "                                                                          args['singleLettersFile_'+str(dayIdx)],\n",
    "                                                                          args['labelsFile_'+str(dayIdx)],\n",
    "                                                                          args['cvPartitionFile_'+str(dayIdx)],\n",
    "                                                                          args['sessionName_'+str(dayIdx)],\n",
    "                                                                          args['rnnBinSize'],\n",
    "                                                                          args['timeSteps'],\n",
    "                                                                          args['isTraining'])\n",
    "    realDataSize = args['batchSize'] - args['synthBatchSize']\n",
    "    trainIdx = cvIdx['trainIdx']\n",
    "    valIdx = cvIdx['testIdx']\n",
    "    print('create real dataset ', dayIdx)\n",
    "    realData_train = handBCI_Dataset(args,neuralData[trainIdx,:,:], targets[trainIdx,:,:], errWeights[trainIdx,:],\\\n",
    "                               binsPerTrial[trainIdx,np.newaxis],\\\n",
    "                               addNoise=True)\n",
    "    realDataTrain_Loader = torch.utils.data.DataLoader(realData_train, batch_size =realDataSize,shuffle=True, num_workers=0)\n",
    "    \n",
    "    if len(valIdx)==0:\n",
    "        realDataVal_Loader = realDataTrain_Loader\n",
    "    else:\n",
    "        realData_val = handBCI_Dataset(args,neuralData[valIdx,:,:], targets[valIdx,:,:], errWeights[valIdx,:],\\\n",
    "                                   binsPerTrial[valIdx,np.newaxis],\\\n",
    "                                       addNoise=False)\n",
    "        realDataVal_Loader = torch.utils.data.DataLoader(realData_val, batch_size =args['batchSize'],shuffle=True, num_workers=0)\n",
    "        daysWithValData.append(dayIdx)\n",
    "    allRealDataLoaders.append(realDataTrain_Loader)\n",
    "    allValDataLoaders.append(realDataVal_Loader)\n",
    "              \n",
    "    ## sythetic data\n",
    "    if args['synthBatchSize'] > 0:\n",
    "        print('processing sythetic data ', dayIdx)\n",
    "        synthDir = args['syntheticDatasetDir_'+str(dayIdx)]\n",
    "        synth_obj = handBCI_SythDataset(synthDir, args)\n",
    "        synth_ds = synth_obj.makeDataSet()\n",
    "        synth_loader = torch.utils.data.DataLoader(synth_ds, batch_size=args['synthBatchSize'])\n",
    "        allSynthDataLoaders.append(synth_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniBatch = next(iter(allRealDataLoaders[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charSeqNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(charSeqNet, self).__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        #count how many days of data are specified\n",
    "        self.nDays = args['nDays']\n",
    "        self.args = args\n",
    "        if self.args['seed']==-1:\n",
    "            self.args['seed']=datetime.now().microsecond\n",
    "        drop_prob = args['drop_prob']\n",
    "        #define the dimensions of layers in the RNN\n",
    "        self.batchSize = args['batchSize']\n",
    "        nOutputs = 31 + 1 # 31 letters/punctations + 1 transition labels\n",
    "        self.nInputs = 192        \n",
    "        self.nUnits = 512 # args['nUnits']\n",
    "        nUnits2 = 512\n",
    "        self.nTimeSteps = args['timeSteps']\n",
    "        self.rnnBinSize = args['rnnBinSize']\n",
    "        self.skipLen = args['skipLen']\n",
    "        self.outputDelay = args['outputDelay']\n",
    "        self.inputLayers = {}\n",
    "        nLinerOuput = 192\n",
    "        for j in range(self.nDays):\n",
    "            self.inputLayers['input_'+str(j)] = nn.Linear(self.nInputs, nLinerOuput, bias = True)\n",
    "\n",
    "        self.input = self.inputLayers['input_0']\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        self.gru1 = nn.GRU(nLinerOuput, self.nUnits, 1, \\\n",
    "                                 batch_first=True, dropout=0)\n",
    "        self.gru2 = nn.GRU(self.nUnits, nUnits2, 1, \\\n",
    "                             batch_first=True, dropout=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(nUnits2, nOutputs, bias = True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "#       x shape: [args['batchSize'], args['timeSteps'], nInputs]        \n",
    "        x = torch.transpose(x, 1,2) ## swap time dimension with channel dimenion to get [bsize, chan, time]\n",
    "        if self.args['smoothInputs']==1: ## smooth\n",
    "            x = gaussSmooth(x, kernelSD=4/self.rnnBinSize)\n",
    "        x = torch.transpose(x, 1,2) #swap time dimension with channel dimenion to get [bsize, time,chan]\n",
    "        x = x.clone().float().detach().requires_grad_(True)\n",
    "#         x = Variable(x.clone().float().detach(), requires_grad=True)\n",
    "        x = self.input(x.to('cuda'))\n",
    "#         x = self.relu1(x)  ## Add a relu layer\n",
    "#         h0 = self.init_hidden(self.batchSize)\n",
    "        x, h = self.gru1(x)\n",
    "        x, h = self.gru2(x[:, 0::self.skipLen,:], h)  # downsample x's time dimension\n",
    "#         x = torch.repeat_interleave(x.detach(), self.skipLen, dim=1)## upsample x's time dimension\n",
    "        output = self.fc1(torch.repeat_interleave(x.detach(), self.skipLen, dim=1)) ## upsample x's time dimension\n",
    "        return output\n",
    "        ## Weights initialization\n",
    "\n",
    "    def init_hidden(self,  batch_size):\n",
    "        hidden = self.gru1.weight_hh_l0.new(1, batch_size, self.nUnits).zero_().to('cuda')\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Network    2713824\n"
     ]
    }
   ],
   "source": [
    "## Uncomment the lines below to train your network\n",
    "charSeq_net = charSeqNet(args).to(DEVICE)\n",
    "## initialization \n",
    "for p in charSeq_net.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "#         nn.init.xavier_normal_(p.weight)\n",
    "#         p.bias.data.zero_()\n",
    "# for ii, layer in enumerate(charSeq_net.parameters()):\n",
    "#     if type(layer) == nn.Linear:\n",
    "#         nn.init.xavier_uniform_(layer, gain=nn.init.calculate_gain('relu'))\n",
    "print(\"Total Parameters in Network {:10d}\".format(sum(p.numel() for p in charSeq_net.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charSeqNet(\n",
      "  (input): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (gru1): GRU(192, 512, batch_first=True)\n",
      "  (gru2): GRU(512, 512, batch_first=True)\n",
      "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(charSeq_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 1200, 192]          37,056\n",
      "               GRU-2  [[-1, 1200, 512], [-1, 2, 512]]               0\n",
      "               GRU-3  [[-1, 240, 512], [-1, 2, 512]]               0\n",
      "            Linear-4             [-1, 1200, 32]          16,416\n",
      "================================================================\n",
      "Total params: 53,472\n",
      "Trainable params: 53,472\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.88\n",
      "Forward/backward pass size (MB): 5757.95\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 5759.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "netSummary(charSeq_net,input_size = (1200,192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_letter = nn.CrossEntropyLoss(reduction = 'none') # we need to apply error weight matrix before averaging\n",
    "transit_sigmoid = nn.Sigmoid()\n",
    "criterion_transit = nn.MSELoss() ## mean square error for transit\n",
    "optimizer = torch.optim.AdamW(charSeq_net.parameters(),lr=1e-4, betas=(0.9, 0.999), eps=1e-08,\\\n",
    "                           weight_decay = args['l2scale'], amsgrad = False) # weight_decay for l2 regurlization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFrameAccuracy(rnnOutput, targets, errWeight, outputDelay):\n",
    "    \"\"\"\n",
    "    Computes a frame-by-frame accuracy percentage given the rnnOutput and the targets, while ignoring\n",
    "    frames that are masked-out by errWeight and accounting for the RNN's outputDelay. \n",
    "    \"\"\"\n",
    "    #Select all columns but the last one (which is the character start signal) and align rnnOutput to targets\n",
    "    #while taking into account the output delay. \n",
    "    bestClass = np.argmax(rnnOutput[:,outputDelay:,0:-1], axis=2)\n",
    "    indicatedClass = np.argmax(targets[:,0:-outputDelay,0:-1], axis=2)\n",
    "    bw = errWeight[:,0:-outputDelay]\n",
    "\n",
    "    #Mean accuracy is computed by summing number of accurate frames and dividing by total number of valid frames (where bw == 1)\n",
    "    acc = np.sum(bw*np.equal(np.squeeze(bestClass), np.squeeze(indicatedClass)))/np.sum(bw)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _validationDiagnostics(self, i, nBatchesPerVal, lr, totalSeconds, runResultsTrain, trainAcc):\n",
    "#     \"\"\"\n",
    "#     Runs a single minibatch on the validation data and returns performance statistics and a snapshot of key variables for\n",
    "#     diagnostic purposes. The snapshot file can be loaded and plotted by an outside program for real-time feedback of how\n",
    "#     the training process is going.\n",
    "#     \"\"\"\n",
    "#     #Randomly select a day that has validation data; if there is no validation data, then just use the last days' training data\n",
    "#     if self.daysWithValData==[]:\n",
    "#         dayNum = self.nDays-1\n",
    "#         datasetNum = dayNum*2\n",
    "#     else:\n",
    "#         randIdx = np.random.randint(len(self.daysWithValData))\n",
    "#         dayNum = self.daysWithValData[randIdx]\n",
    "#         datasetNum = 1+dayNum*2 #odd numbers are the validation partitions\n",
    "\n",
    "#     runResults = self._runBatch(datasetNum=datasetNum, dayNum=dayNum, lr=lr, computeGradient=True, doGradientUpdate=False)\n",
    "\n",
    "#     valAcc = computeFrameAccuracy(runResults['logitOutput'], \n",
    "#                             runResults['targets'],\n",
    "#                             runResults['batchWeight'], \n",
    "#                             self.args['outputDelay'])\n",
    "\n",
    "#     print('Val Batch: ' + str(i) + '/' + str(self.args['nBatchesToTrain']) + ', valErr: ' + str(runResults['err']) + ', trainErr: ' + str(runResultsTrain['err']) + ', Val Acc.: ' + str(valAcc) + ', Train Acc.: ' + str(trainAcc) + ', grad: ' + str(runResults['gradNorm']) + ', learnRate: ' + str(lr) + ', time: ' + str(totalSeconds))\n",
    "\n",
    "#     outputSnapshot = {}\n",
    "#     outputSnapshot['inputs'] = runResults['inputFeatures'][0,:,:]\n",
    "#     outputSnapshot['rnnUnits'] = runResults['output'][0,:,:]\n",
    "#     outputSnapshot['charProbOutput'] = runResults['logitOutput'][0,:,0:-1]\n",
    "#     outputSnapshot['charStartOutput'] = scipy.special.expit(runResults['logitOutput'][0,self.args['outputDelay']:,-1])\n",
    "#     outputSnapshot['charProbTarget'] = runResults['targets'][0,:,0:-1]\n",
    "#     outputSnapshot['charStartTarget'] = runResults['targets'][0,:,-1]\n",
    "#     outputSnapshot['errorWeight'] = runResults['batchWeight'][0,:]\n",
    "\n",
    "#     return [i, runResults['err'], runResults['gradNorm'], valAcc], outputSnapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_lambda = lambda epoch: args['learnRateStart']*(1 - epoch/args['nBatchesToTrain'])\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/17\n",
    "#  auto grad\n",
    "#https://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " ## Traing..\n",
    "nPredicts = args['timeSteps']-args['outputDelay']\n",
    "train_acc = []\n",
    "# dayIdx = np.random.randint(2) #args['nDays']\n",
    "dtStart = datetime.now()\n",
    "scale_transit_loss = 5\n",
    "dayIdx = 6 #np.random.randint(2)\n",
    "for epoch in range(args['nBatchesToTrain']):\n",
    "#     dtStart = datetime.now()\n",
    "\n",
    "    # grab a batch of data\n",
    "#     miniBatch = next(iter(allRealDataLoaders[dayIdx]))\n",
    "#     X = miniBatch['inputs']\n",
    "#     target_raw = miniBatch['labels']\n",
    "#     erws = miniBatch['errWeights'] \n",
    "    \n",
    "    X, target_raw, erws = combineSynthAndReal(iter(allSynthDataLoaders[dayIdx]),iter(allRealDataLoaders[dayIdx])) ## change 0 to dayIdx in full mode\n",
    "#     totalSeconds = (datetime.now()-dtStart).total_seconds()\n",
    "\n",
    "#     print('data loading time:', totalSeconds)\n",
    "\n",
    "    charSeq_net.input = charSeq_net.inputLayers['input_'+str(dayIdx)].to('cuda') ## day specific input\n",
    "    optimizer.zero_grad() ## clear gradients befor new forward pass\n",
    "    output = charSeq_net(X) ## forward pass\n",
    "\n",
    "    ## here we accounting for the output delay\n",
    "    target = target_raw[:,0:-args['outputDelay'],:]\n",
    "    bw = erws[:,0:-args['outputDelay']]\n",
    "    logits = output[:,args['outputDelay']:,:]\n",
    "    \n",
    "    ## seperate out characters and transit signal (last column)\n",
    "    output_transit = Variable(logits[:,:,-1].to('cuda'), requires_grad=True)\n",
    "    output_letters = Variable(logits[:,:,0:-1].to('cuda'), requires_grad=True)\n",
    "    target_transit = Variable(target[:,:,-1].to('cuda'), requires_grad=True)\n",
    "    target_letters = Variable(target[:,:,0].to('cuda'), requires_grad=True)\n",
    "    \n",
    "#     output_ = output[:,args['outputDelay']:,:][:,:,-1]\n",
    "#     target_ = target_raw[:,0:-args['outputDelay'],:][:,:,-1]\n",
    "    ## compute loss\n",
    "    loss_letters=[]\n",
    "    for t in range(args['timeSteps']-args['outputDelay']):  ## need to apply the error weight per timestep per batch \n",
    "        loss_letters.append(criterion_letter(output_letters[:,t,:],\\\n",
    "                                             target_letters[:,t].long())*bw[:,t].to('cuda')/nPredicts)\n",
    "  \n",
    "    #loss_letters.backward(retain_graph=True)\n",
    "    loss_letters = torch.stack(loss_letters, dim=0).sum(dim=0).mean(dim=0) ## sum over averaged time then mean across batch\n",
    "    loss_transit = criterion_transit(transit_sigmoid(output[:,args['outputDelay']:,:][:,:,-1]),\\\n",
    "                                     target_raw[:,0:-args['outputDelay'],:][:,:,-1].float().to('cuda'))\n",
    "    loss = loss_letters + scale_transit_loss*loss_transit\n",
    "\n",
    "    ## backprop\n",
    "    loss.backward()\n",
    "\n",
    "#     clip_grad_norm_(charSeq_net.parameters(), args['clip_grads']) ## clip gradient to prevent explosion\n",
    "    optimizer.step()          # weight update\n",
    "#     print(charSeq_net.fc1.weight.data.grad)\n",
    "#     print(charSeq_net.input.weight.data.grad)\n",
    "    scheduler.step()          ## learning rate update\n",
    "    \n",
    "    ## report \n",
    "\n",
    "    if epoch % args['batchesPerVal'] == 0:\n",
    "        with torch.no_grad():\n",
    "            trainAcc = computeFrameAccuracy(output.detach().cpu().numpy(), \n",
    "                            target_raw.detach().cpu().numpy(),\n",
    "                            erws.detach().cpu().numpy(), \n",
    "                            args['outputDelay'])\n",
    "            totalSeconds = (datetime.now()-dtStart).total_seconds()\n",
    "            train_acc.append(trainAcc)\n",
    "#             totalPars = sum(p.numel() for p in charSeq_net.parameters())\n",
    "            print(f'epoc#{str(epoch):3} day#{str(dayIdx):2} train-loss:{loss.item():.3f} train-Acc:{trainAcc:.2%} train-time:{totalSeconds/60.0:.1f}')\n",
    "\n",
    "#         batchValStats[valSetIdx,0:4], outputSnapshot = \\\n",
    "#         self._validationDiagnostics(i, args['batchesPerVal'], lr,\\\n",
    "#                                     totalSeconds, runResultsTrain, trainAcc)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "val_epochs = np.arange(0, len(train_acc)*args['batchesPerVal'],args['batchesPerVal'])\n",
    "plt.plot(val_epochs,train_acc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(charSeq_net.input.weight.grad, charSeq_net.input.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(charSeq_net.gru1.weights.grad, charSeq_net.gru1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
