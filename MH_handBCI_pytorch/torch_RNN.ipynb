{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from torch_setupParameters import set_seed,set_device, getDefaultRNNArgs\n",
    "from torch_dataPreprocessing import loadAllRealDatasets, prepareDataCubesForRNN\n",
    "from torch_dataPreprocessing import normalizeSentenceDataCube, binTensor\n",
    "from torch_dataPreprocessing import handBCI_Dataset, handBCI_SythDataset, gaussSmooth\n",
    "from tfrecord.torch.dataset import MultiTFRecordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point this towards the top level dataset directory\n",
    "rootDir = '../handwritingBCIData/'\n",
    "outDir = 'output/'\n",
    "#train an RNN using data from these specified sessions\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "\n",
    "#use this train/test partition \n",
    "cvPart = 'HeldOutTrials'\n",
    "\n",
    "#name of the directory where this RNN run will be saved\n",
    "rnnOutputDir = cvPart\n",
    "\n",
    "## parameters\n",
    "args = getDefaultRNNArgs(rootDir, cvPart, outDir)\n",
    "#Configure the arguments for a multi-day RNN (that will have a unique input layer for each day)\n",
    "for x in range(len(dataDirs)):\n",
    "    args['sentencesFile_'+str(x)] = rootDir+'Datasets/'+dataDirs[x]+'/sentences.mat'\n",
    "    args['singleLettersFile_'+str(x)] = rootDir+'Datasets/'+dataDirs[x]+'/singleLetters.mat'\n",
    "    args['labelsFile_'+str(x)] = rootDir+'RNNTrainingSteps/Step2_HMMLabels/'+cvPart+'/'+dataDirs[x]+'_timeSeriesLabels.mat'\n",
    "    args['syntheticDatasetDir_'+str(x)] = rootDir+'Datasets/'+dataDirs[x]+'/'+cvPart+'/'+dataDirs[x]+'_syntheticSentences/'\n",
    "    args['cvPartitionFile_'+str(x)] = rootDir+'RNNTrainingSteps/trainTestPartitions_'+cvPart+'.mat'\n",
    "    args['sessionName_'+str(x)] = dataDirs[x]\n",
    "\n",
    "for t in range(30):  ## 10 days\n",
    "    if 'labelsFile_'+str(t) not in args.keys():\n",
    "        args['nDays'] = t\n",
    "        break\n",
    "if not os.path.isdir(args['outputDir']):\n",
    "    os.mkdir(args['outputDir'])\n",
    "    \n",
    "#this weights each day equally (0.1 probability for each day) and allocates a unique input layer for each day (0-9)\n",
    "args['dayProbability'] = '[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]'\n",
    "args['dayToLayerMap'] = '[0,1,2,3,4,5,6,7,8,9]'\n",
    "# args['verbose'] = True ## extra print-out information\n",
    "\n",
    "args['mode'] = 'train' ## make sure it is set in 'train' mode\n",
    "print('batchSize:', args['batchSize'])\n",
    "print('synthBatchSize:', args['synthBatchSize'])\n",
    "args['ForTestingOnly'] = False ## FOR DEBUGING. set \"self.nDays = 2\" (use 2 days of data for testing run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSynthDataLoaders = []\n",
    "allRealDataLoaders = []\n",
    "allValDataLoaders = []\n",
    "daysWithValData = []\n",
    "args['isTraining'] = True\n",
    "for dayIdx in [2,3]: #range(args['nDays']):\n",
    "    ## real data\n",
    "    print('Loading real data ', dayIdx)\n",
    "    neuralData, targets, errWeights, binsPerTrial, cvIdx = prepareDataCubesForRNN(args['sentencesFile_'+str(dayIdx)],\n",
    "                                                                          args['singleLettersFile_'+str(dayIdx)],\n",
    "                                                                          args['labelsFile_'+str(dayIdx)],\n",
    "                                                                          args['cvPartitionFile_'+str(dayIdx)],\n",
    "                                                                          args['sessionName_'+str(dayIdx)],\n",
    "                                                                          args['rnnBinSize'],\n",
    "                                                                          args['timeSteps'],\n",
    "                                                                          args['isTraining'])\n",
    "    realDataSize = args['batchSize'] - args['synthBatchSize']\n",
    "    trainIdx = cvIdx['trainIdx']\n",
    "    valIdx = cvIdx['testIdx']\n",
    "    print('create real dataset ', dayIdx)\n",
    "    realData_train = handBCI_Dataset(args,neuralData[trainIdx,:,:], targets[trainIdx,:,:], errWeights[trainIdx,:],\\\n",
    "                               binsPerTrial[trainIdx,np.newaxis],\\\n",
    "                               addNoise=True)\n",
    "    realDataTrain_Loader = torch.utils.data.DataLoader(realData_train, batch_size =realDataSize,shuffle=True, num_workers=0)\n",
    "    \n",
    "    if len(valIdx)==0:\n",
    "        realDataVal_Loader = realDataTrain_Loader\n",
    "    else:\n",
    "        realData_val = handBCI_Dataset(args,neuralData[valIdx,:,:], targets[valIdx,:,:], errWeights[valIdx,:],\\\n",
    "                                   binsPerTrial[valIdx,np.newaxis],\\\n",
    "                                       addNoise=False)\n",
    "        realDataVal_Loader = torch.utils.data.DataLoader(realData_val, batch_size =args['batchSize'],shuffle=True, num_workers=0)\n",
    "        daysWithValData.append(dayIdx)\n",
    "    allRealDataLoaders.append(realDataTrain_Loader)\n",
    "    allValDataLoaders.append(realDataVal_Loader)\n",
    "              \n",
    "    ## sythetic data\n",
    "    print('processing sythetic data ', dayIdx)\n",
    "    synthDir = args['syntheticDatasetDir_'+str(dayIdx)]\n",
    "    synth_obj = handBCI_SythDataset(synthDir, args)\n",
    "    synth_ds = synth_obj.makeDataSet()\n",
    "    synth_loader = torch.utils.data.DataLoader(synth_ds, batch_size=args['synthBatchSize'])\n",
    "    allSynthDataLoaders.append(synth_loader)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charSeqNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(charSeqNet, self).__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        #count how many days of data are specified\n",
    "        self.nDays = args['nDays']\n",
    "        self.args = args\n",
    "        if self.args['seed']==-1:\n",
    "            self.args['seed']=datetime.now().microsecond\n",
    "        drop_prob = args['drop_prob']\n",
    "        #define the dimensions of layers in the RNN\n",
    "        nOutputs = 31\n",
    "        nInputs = 192        \n",
    "        nUnits = args['nUnits']\n",
    "        nTimeSteps = args['timeSteps']\n",
    "        self.rnnBinSize = args['rnnBinSize']\n",
    "        inputLayers = []        \n",
    "#        shape: [args['batchSize'], args['timeSteps'], nInputs]\n",
    "        for j in range(self.nDays):\n",
    "            inputLayers.append(nn.Linear(nInputs, nInputs, bias = True))\n",
    "\n",
    "        self.inputLayers = inputLayers\n",
    "        self.gru1 = torch.nn.GRU(nInputs, nUnits, 1, \\\n",
    "                                 batch_first=True, dropout=drop_prob)\n",
    " \n",
    "        self.gru2 = torch.nn.GRU(nUnits, nUnits, 1, \\\n",
    "                                 batch_first=True, dropout=drop_prob)\n",
    "\n",
    "        self.fc1 = nn.Linear(nUnits, nOutputs, bias = True)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "    def forward(self, x, dayIdx):\n",
    "\n",
    "        if self.args['smoothInputs']==1: ## smooth\n",
    "            x = torch.Tensor(gaussSmooth(x, kernelSD=4/self.rnnBinSize))\n",
    "        layer1 = self.inputLayers[dayIdx].to('cuda') ## day specific input layer\n",
    "        x = layer1(x.to('cuda'))\n",
    "        x, h = self.gru1(x)\n",
    "        ## TODO: downsample x's time dimension\n",
    "        x, h = self.gru2(x)\n",
    "        ## TODO: upsample\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def train(self):\n",
    "        ## TO be continued ...\n",
    "        n = 100\n",
    "        for epoc in range(n):\n",
    "            dayIdx = np.random.randint(self.nDays)\n",
    "            miniBatch = next(iter(allRealDataLoaders[0])) ## change 0 to dayIdx in full mode\n",
    "            X = miniBatch['inputs']\n",
    "            target = miniBatch['labels']\n",
    "            er = miniBatch['errWeights']\n",
    "            output = self.forward(X, dayIdx)\n",
    "            ## output is 32 dimensions, target is 31?\n",
    "            loss = self.criterion(output, target)\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# miniBatch = next(iter(allRealDataLoaders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the lines below to train your network\n",
    "charSeq_net = charSeqNet(args).to(DEVICE)\n",
    "print(\"Total Parameters in Network {:10d}\".format(sum(p.numel() for p in charSeq_net.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charSeq_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
